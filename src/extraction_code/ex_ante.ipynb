{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41828c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import undetected_chromedriver as uc\n",
    "import locale\n",
    "\n",
    "import calendar\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c21b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "months = {\n",
    "    \"ene\": \"Jan\", \"feb\": \"Feb\", \"mar\": \"Mar\", \"abr\": \"Apr\",\n",
    "    \"may\": \"May\", \"jun\": \"Jun\", \"jul\": \"Jul\", \"ago\": \"Aug\",\n",
    "    \"sept\": \"Sep\", \"oct\": \"Oct\", \"nov\": \"Nov\", \"dic\": \"Dec\"\n",
    "}\n",
    "\n",
    "locale.setlocale(locale.LC_TIME, 'C')\n",
    "\n",
    "json_pages_info = {\"pages\": []}\n",
    "\n",
    "\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "pages = 10\n",
    "\n",
    "for page in range(0, pages):\n",
    "\n",
    "\turl = f'https://www.google.com/search?q=%22Estallido+social%22+site%3Awww.ex-ante.cl&tbs=cdr:1,cd_min:11/15/2019,cd_max:12/17/2023&start={page * 10}'\n",
    "\n",
    "\tdriver.get(url)\n",
    "\n",
    "\ttime.sleep(2)\n",
    "\n",
    "\tarticles_section = driver.find_element(By.CLASS_NAME, \"dURPMd\")\n",
    "\tarticles = articles_section.find_elements(By.CLASS_NAME, \"MjjYud\")\n",
    "\n",
    "\tfor article in articles:\n",
    "\t\tprint(\"revisando artículo...\")\n",
    "\t\ttry: \n",
    "\t\t\toriginalDate = article.find_element(By.CLASS_NAME, \"YrbPuc\").find_element(By.TAG_NAME, \"span\").text\n",
    "\t\t\tfor es, en in months.items():\n",
    "\t\t\t\tif es in originalDate:\n",
    "\t\t\t\t\t\toriginalDate = originalDate.replace(es, en)\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\n",
    "\t\t\tdate_epoch = int(time.mktime(time.strptime(originalDate, \"%d %b %Y\")))\n",
    "\n",
    "\t\t\tif (date_epoch < 1573786800 or date_epoch > 1702782000):\n",
    "\t\t\t\tprint(\"La noticia no corresponde a la fecha solicitada\")\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\ttry:\n",
    "\t\t\t\ttitle = article.find_element(By.TAG_NAME, \"h3\").text\n",
    "\t\t\t\tdescription = article.find_element(By.CLASS_NAME, \"kb0PBd \").find_elements(By.TAG_NAME, \"span\")[1].text\n",
    "\t\t\t\tlink = article.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "\n",
    "\t\t\t\tlink_info = {\n",
    "\t\t\t\t\t\"newscast\" : \"El Mostrador\",\n",
    "\t\t\t\t\t\"title\": title,\n",
    "\t\t\t\t\t\"description\": description,\n",
    "\t\t\t\t\t\"category\": \"The site does not provide a category\",\n",
    "\t\t\t\t\t\"date\": originalDate,\n",
    "\t\t\t\t\t\"image_link\": \"not found initially\",\n",
    "\t\t\t\t\t\"author\": \"not found initially\",\n",
    "\t\t\t\t\t\"link\": link,\n",
    "\t\t\t\t}\n",
    "\n",
    "\t\t\t\tprint(\"información incluida!\")\n",
    "\t\t\t\tjson_pages_info[\"pages\"].append(link_info)\n",
    "\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f\"Error al extraer información del artículo: {e}\")\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error al procesar el artículo: {e}\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "print(\"Almacenando información en el archivo JSON...\")\n",
    "with open(f\"../archive/temp/el_mostrador_pages.json\", 'w', encoding='utf-8') as file:\n",
    "\tjson.dump(json_pages_info, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "with open(f\"output.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "\tf.write(driver.page_source)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c04755",
   "metadata": {},
   "source": [
    "## Extraction of content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ede08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extracted_pages = {\"pages\": []}\n",
    "extracted_pages_with_content = {\"pages\": []}\n",
    "total_pages = 0\n",
    "pages_succeeded = 0\n",
    "pages_failed = 0\n",
    "error_occurred = False\n",
    "driver = None\n",
    "ad_message = \"Suscríbase aquí a la newsletter de EL PAÍS América y reciba todas las claves informativas de la actualidad de la región.\"\n",
    "limit_of_pages = \"all\"\n",
    "start_page = 10\n",
    "\n",
    "content = \"\"\n",
    "author = \"\"\n",
    "\n",
    "print(\"[INFO] Loading extracted pages from JSON file...\")\n",
    "with open(\"../archive/pages_extracted/ex_ante/ex_ante_pages.json\", 'r', encoding='utf-8') as file:\n",
    "\t\textracted_pages = json.load(file)\n",
    "\t\tprint(f\"[INFO] {len(extracted_pages['pages'])} pages loaded.\")\n",
    "\t\tif limit_of_pages == \"all\":\n",
    "\t\t\tlimit_of_pages = len(extracted_pages[\"pages\"])\n",
    "\n",
    "\n",
    "if start_page >= len(extracted_pages[\"pages\"]):\n",
    "\t\tprint(f\"[ERROR] start_page is out of range. The maximum page index is {len(extracted_pages['pages']) - 1}.\")\n",
    "\t\traise SystemExit\n",
    "\n",
    "def extract_from_DOM():\n",
    "\t\tprint(\"[INFO] Trying to extract content from DOM...\")\n",
    "\t\tcontent = \"\"\n",
    "\t\tauthor = \"\"\n",
    "\n",
    "\t\tcontent_container = driver.find_element(By.CLASS_NAME, \"contenido-noticia\")\n",
    "\t\tparagraph = content_container.find_elements(By.TAG_NAME, \"p\") if content_container else None\n",
    "\t\tcontent = \"\"\n",
    "\t\tfor p in paragraph:\n",
    "\t\t\tp = p.text.strip()\n",
    "\t\t\tif \"También puede leer.\" in p or \"Lea también:\" in p or \"Para seguir leyendo columnas de Ex-Ante, clic aquí.\" in p:\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\tcontent += p + \"\\n\"\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\t\tauthor = driver.find_element(By.CLASS_NAME, \"autor\").text.strip()\n",
    "\t\texcept:\n",
    "\t\t\t\tauthor = \"not found\"\n",
    "\n",
    "\t\tprint(\"[INFO] The content has been extracted. Method: DOM\", flush=True)\n",
    "\t\tif len(content) < 100:\n",
    "\t\t\t\tprint(\"[WARNING] The extracted content is too short.\", flush=True)\n",
    "\t\t\t\tprint(f\"[WARNING] Content: {content}\", flush=True)\n",
    "\t\treturn content, author\n",
    "\n",
    "data_extracted = True\n",
    "re = False\n",
    "\n",
    "print(\"[INFO] Starting extraction process...\")\n",
    "for page_index, page in enumerate(extracted_pages[\"pages\"]):\n",
    "\n",
    "\t\tif \"content\" not in page.keys():\n",
    "\t\t\t\tpage[\"content\"] = \"The content has not been extracted yet\"\n",
    "\n",
    "\t\tif page_index < start_page:\n",
    "\t\t\t\tprint(\"[INFO] Skipping page: \", page_index)\n",
    "\t\t\t\tcontinue\n",
    "\t\tif limit_of_pages == total_pages:\n",
    "\t\t\t\tprint(\"[INFO] Limit of pages reached.\")\n",
    "\t\t\t\tbreak\n",
    "\t\tif page[\"content\"] != \"The content has not been extracted yet\":\n",
    "\t\t\t\tprint(f\"[INFO] Page {page_index} already has content. Skipping...\")\n",
    "\t\t\t\textracted_pages_with_content[\"pages\"].append(page)\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\ttotal_pages += 1\n",
    "\n",
    "\t\tprint(f\"\\n[INFO] Checking page: {page_index} | Link: {page['link']}\" )\n",
    "\t\turl = page['link']\n",
    "\t\tfirst = True\n",
    "\n",
    "\t\twhile (re or first):\n",
    "\t\t\tfirst = False\n",
    "\t\t\t\n",
    "\t\t\ttry: \n",
    "\t\t\t\ttime.sleep(1)\n",
    "\n",
    "\t\t\t\tif (not re): \n",
    "\t\t\t\t\toptions = uc.ChromeOptions()\n",
    "\t\t\t\t\toptions.add_argument(\"--no-sandbox\")\n",
    "\t\t\t\t\toptions.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "\t\t\t\t\tdriver = uc.Chrome(options=options)\n",
    "\n",
    "\t\t\t\t\tdriver.get(url)\n",
    "\n",
    "\t\t\t\tre = False\n",
    "\t\t\t\ttime.sleep(random.uniform(6, 12))\n",
    "\n",
    "\t\t\t\ttry: \n",
    "\t\t\t\t\tcontent, author = extract_from_DOM()\n",
    "\t\t\t\t\tif author == \"not found\":\n",
    "\t\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\t\t\tauthor = driver.find_element(By.CLASS_NAME, \"a_md_a\").text.strip()\n",
    "\t\t\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\t\t\t\tauthor = \"not found\"\n",
    "\n",
    "\t\t\t\t\t# Clean up ad message\n",
    "\t\t\t\t\tcontent = content.replace(ad_message, \"\").strip()\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tprint(f\"[ERROR] A error occurred while extracting content\", flush=True)\n",
    "\t\t\t\t\traise\n",
    "\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f\"[ERROR] A error occurred while processing the page: {page_index}\", flush=True)\n",
    "\t\t\t\tprint(f\"[ERROR] Site: {url}\", flush=True)\n",
    "\t\t\t\tprint(e, flush=True)\n",
    "\t\t\t\tresponse = input(\"Retry? y/n\")\n",
    "\t\t\t\tif response.lower() == 'y':\n",
    "\t\t\t\t\tprint(\"[INFO] Retry...\")\n",
    "\t\t\t\t\tre = True\n",
    "\t\t\t\telse: \n",
    "\t\t\t\t\tprint(\"[INFO] Cancelling...\")\n",
    "\t\t\t\t\terror_occurred = True\n",
    "\t\t\t\t\tre = False\n",
    "\t\t\t\n",
    "\t\t\tif (not re):\n",
    "\t\t\t\tdriver.quit()\n",
    "\n",
    "\t\tif error_occurred:\n",
    "\t\t\tcontent = \"A error occurred while extracting content\"\n",
    "\t\t\tauthor = \"A error occurred while extracting content\"\n",
    "\t\t\tpages_failed += 1\n",
    "\t\t\terror_occurred = False\n",
    "\t\telse:\n",
    "\t\t\tpages_succeeded += 1\n",
    "\n",
    "\t\tpage[\"content\"] = content\n",
    "\t\tpage[\"author\"] = author\n",
    "\t\textracted_pages_with_content[\"pages\"].append(page)\n",
    "\n",
    "\t\tprint(\"[INFO] Storing information in a JSON file...\")\n",
    "\t\twith open(f\"../archive/temp/pages_with_content/ex_ante_pages_with_content.json\", 'w', encoding='utf-8') as file:\n",
    "\t\t\tjson.dump(extracted_pages_with_content, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\t\tdriver.quit()\n",
    "\n",
    "\t\ttime.sleep(random.uniform(6, 12))\n",
    "\n",
    "# ----------------- Stats -----------------\n",
    "print(\"\\n\\n\")\n",
    "print(\"----- STATS -----\")\n",
    "\n",
    "try:\n",
    "\tprint(f\"Amount of pages processed: {total_pages}\")\n",
    "\tprint(f\"% Pages succeeded [{pages_succeeded}]: {round(100 * pages_succeeded/total_pages, 2)}\")\n",
    "\tprint(f\"% Pages failed [{pages_failed}]: {round(100 * pages_failed/total_pages, 2)}\")\n",
    "except: \n",
    "\tprint(\"A error occurred while processing the stats\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
