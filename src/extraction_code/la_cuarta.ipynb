{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e3f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extraction import extract_articles_from_google\n",
    "from extraction import extract_data_from_page\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc7a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_file_name = \"la_cuarta_articles.json\"\n",
    "content_file_name = \"la_cuarta_articles_with_content.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef31ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_articles_from_google(\n",
    "\twebsite=\"lacuarta.com\",\n",
    "\tnewscast=\"La Cuarta\",\n",
    "    pages=2,\n",
    "    output=pages_file_name,\n",
    "    keywords=\"Estallido Social\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f5a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(body: WebElement):\n",
    "\tcontent = \"\"\n",
    "\tfor element in body.find_elements(By.CLASS_NAME, \"article-body__paragraph\"):\n",
    "\t\tcontent += element.text.strip() + \"\\n\"\n",
    "\treturn content\n",
    "\n",
    "def get_author(body: WebElement):\n",
    "\ttry:\n",
    "\t\treturn body.find_element(By.CLASS_NAME, \"article-body__byline__author\").text.strip()\n",
    "\texcept:\n",
    "\t\treturn \"not found\"\n",
    "\n",
    "def get_description(body: WebElement):\n",
    "\ttry:\n",
    "\t\treturn body.find_element(By.CLASS_NAME, \"article-head__subtitle\").text.strip()\n",
    "\texcept:\n",
    "\t\treturn \"not found\"\n",
    "\n",
    "\n",
    "extract_data_from_page(\n",
    "\tinput_file=pages_file_name, \n",
    "\toutput_file=content_file_name,\n",
    "\tget_author=get_author,\n",
    "\tget_description=get_description,\n",
    "\tget_content=get_content,\n",
    "\tlimit_of_pages=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd14149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_pages_info = {\"pages\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6c0f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import random\n",
    "\n",
    "months = {\n",
    "    \"ene\": \"Jan\", \"feb\": \"Feb\", \"mar\": \"Mar\", \"abr\": \"Apr\",\n",
    "    \"may\": \"May\", \"jun\": \"Jun\", \"jul\": \"Jul\", \"ago\": \"Aug\",\n",
    "    \"sept\": \"Sep\", \"oct\": \"Oct\", \"nov\": \"Nov\", \"dic\": \"Dec\"\n",
    "}\n",
    "\n",
    "locale.setlocale(locale.LC_TIME, 'C')\n",
    "\n",
    "# json_pages_info = {\"pages\": []}\n",
    "\n",
    "pages = 20\n",
    "\n",
    "re = False\n",
    "\n",
    "for page in range(10, pages):\n",
    "\n",
    "\tfirst = True\n",
    "\twhile (re or first):\n",
    "\t\tfirst = False\n",
    "\t\t\n",
    "\t\ttry: \n",
    "\t\t\ttime.sleep(1)\n",
    "\n",
    "\t\t\tif (not re): \n",
    "\t\t\t\toptions = uc.ChromeOptions()\n",
    "\t\t\t\toptions.add_argument(\"--no-sandbox\")\n",
    "\t\t\t\toptions.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "\t\t\t\tdriver = uc.Chrome(options=options)\n",
    "\n",
    "\t\t\t\turl = f'https://www.google.com/search?q=%22Estallido+social%22+site%3Awww.lacuarta.com&tbs=cdr:1,cd_min:11/15/2019,cd_max:12/17/2023&start={page * 10}'\n",
    "\t\t\t\tdriver.get(url)\n",
    "\n",
    "\t\t\tre = False\n",
    "\t\t\ttime.sleep(random.uniform(6, 12))\n",
    "\n",
    "\t\t\tarticles_section = driver.find_element(By.CLASS_NAME, \"dURPMd\")\n",
    "\t\t\tarticles = articles_section.find_elements(By.CLASS_NAME, \"MjjYud\")\n",
    "\n",
    "\t\t\tfor article in articles:\n",
    "\t\t\t\tprint(\"revisando artículo...\")\n",
    "\t\t\t\ttry: \n",
    "\t\t\t\t\toriginalDate = article.find_element(By.CLASS_NAME, \"YrbPuc\").find_element(By.TAG_NAME, \"span\").text\n",
    "\t\t\t\t\tfor es, en in months.items():\n",
    "\t\t\t\t\t\tif es in originalDate:\n",
    "\t\t\t\t\t\t\t\toriginalDate = originalDate.replace(es, en)\n",
    "\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\tdate_epoch = int(time.mktime(time.strptime(originalDate, \"%d %b %Y\")))\n",
    "\n",
    "\t\t\t\t\tif (date_epoch < 1573786800 or date_epoch > 1702782000):\n",
    "\t\t\t\t\t\tprint(\"La noticia no corresponde a la fecha solicitada\")\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\t# check if the article is from El Mostrador\n",
    "\t\t\t\t\tif (not (\"www.lacuarta.com\" in article.text)):\n",
    "\t\t\t\t\t\tprint(\"El artículo no es de La Cuarta\")\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\ttitle = article.find_element(By.TAG_NAME, \"h3\").text\n",
    "\t\t\t\t\t\tdescription = article.find_element(By.CLASS_NAME, \"kb0PBd \").find_elements(By.TAG_NAME, \"span\")[1].text\n",
    "\t\t\t\t\t\tlink = article.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "\n",
    "\t\t\t\t\t\tlink_info = {\n",
    "\t\t\t\t\t\t\t\"newscast\" : \"La Cuarta\",\n",
    "\t\t\t\t\t\t\t\"title\": title,\n",
    "\t\t\t\t\t\t\t\"description\": description,\n",
    "\t\t\t\t\t\t\t\"category\": \"The site does not provide a category\",\n",
    "\t\t\t\t\t\t\t\"date\": originalDate,\n",
    "\t\t\t\t\t\t\t\"image_link\": \"not found initially\",\n",
    "\t\t\t\t\t\t\t\"author\": \"not found initially\",\n",
    "\t\t\t\t\t\t\t\"link\": link,\n",
    "\t\t\t\t\t\t}\n",
    "\n",
    "\t\t\t\t\t\tprint(\"información incluida!\")\n",
    "\t\t\t\t\t\tjson_pages_info[\"pages\"].append(link_info)\n",
    "\n",
    "\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\tprint(f\"Error al extraer información del artículo: {e}\")\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tprint(f\"Error al procesar el artículo: {e}\")\n",
    "\t\t\t\t\tcontinue\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error al procesar el resultado de busqueda. Página: {page}\")\n",
    "\t\t\tprint(f\"Sitio: {url}\")\n",
    "\t\t\tprint(e)\n",
    "\t\t\tresponse = input(\"Reintentar? y/n\")\n",
    "\t\t\tif response.lower() == 'y':\n",
    "\t\t\t\tprint(\"Reintentando.\")\n",
    "\t\t\t\tre = True\n",
    "\t\t\telse: \n",
    "\t\t\t\tprint(\"Cancelando...\")\n",
    "\t\t\t\tre = False\n",
    "\t\t\n",
    "\t\tif (not re):\n",
    "\t\t\tdriver.quit()\n",
    "\n",
    "print(\"Almacenando información en el archivo JSON...\")\n",
    "with open(f\"../archive/temp/la_cuarta.json\", 'w', encoding='utf-8') as file:\n",
    "\tjson.dump(json_pages_info, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "# with open(f\"output.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "# \tf.write(driver.page_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a17705",
   "metadata": {},
   "source": [
    "# Content Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03629678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import random\n",
    "\n",
    "months = {\n",
    "    \"ene\": \"Jan\", \"feb\": \"Feb\", \"mar\": \"Mar\", \"abr\": \"Apr\",\n",
    "    \"may\": \"May\", \"jun\": \"Jun\", \"jul\": \"Jul\", \"ago\": \"Aug\",\n",
    "    \"sept\": \"Sep\", \"oct\": \"Oct\", \"nov\": \"Nov\", \"dic\": \"Dec\"\n",
    "}\n",
    "\n",
    "locale.setlocale(locale.LC_TIME, 'C')\n",
    "\n",
    "json_pages_info = {\"pages\": []}\n",
    "\n",
    "pages = 1\n",
    "\n",
    "re = False\n",
    "\n",
    "for page in range(0, pages):\n",
    "\n",
    "\tfirst = True\n",
    "\twhile (re or first):\n",
    "\t\tfirst = False\n",
    "\t\t\n",
    "\t\tprint(\"procesando\")\n",
    "\n",
    "\t\ttry: \n",
    "\t\t\ttime.sleep(1)\n",
    "\n",
    "\t\t\tif (not re): \n",
    "\t\t\t\toptions = uc.ChromeOptions()\n",
    "\t\t\t\toptions.add_argument(\"--no-sandbox\")\n",
    "\t\t\t\toptions.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "\t\t\t\tdriver = uc.Chrome(options=options)\n",
    "\n",
    "\t\t\t\turl = f'https://www.lacuarta.com/cronica/noticia/los-50-dias-del-estallido-social/435927/'\n",
    "\t\t\t\tprint(\"obteniendo URL\")\n",
    "\n",
    "\t\t\t\tdriver.get(url)\n",
    "\n",
    "\t\t\tre = False\n",
    "\t\t\tprint(\"Time sleep de 6 a 12\")\n",
    "\t\t\ttime.sleep(random.uniform(6, 12))\n",
    "\n",
    "\t\t\ttitle = driver.find_element(By.TAG_NAME, \"h1\").text\n",
    "\t\t\tprint(f\"Procesando página: {page + 1} - Título: {title}\")\n",
    "\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error al procesar el resultado de busqueda. Página: {page}\")\n",
    "\t\t\tprint(f\"Sitio: {url}\")\n",
    "\t\t\tprint(e)\n",
    "\t\t\tresponse = input(\"Reintentar? y/n\")\n",
    "\t\t\tif response.lower() == 'y':\n",
    "\t\t\t\tprint(\"Reintentando.\")\n",
    "\t\t\t\tre = True\n",
    "\t\t\telse: \n",
    "\t\t\t\tprint(\"Cancelando...\")\n",
    "\t\t\t\tre = False\n",
    "\t\t\n",
    "\t\tif (not re):\n",
    "\t\t\tdriver.quit()\n",
    "\n",
    "#print(\"Almacenando información en el archivo JSON...\")\n",
    "#with open(f\"../archive/temp/la_cuarta.json\", 'w', encoding='utf-8') as file:\n",
    "#\tjson.dump(json_pages_info, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b1e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extracted_pages = {\"pages\": []}\n",
    "extracted_pages_with_content = {\"pages\": []}\n",
    "last_extracted_pages_with_content = {\"pages\": []}\n",
    "\n",
    "total_pages = 0\n",
    "pages_succeeded = 0\n",
    "pages_failed = 0\n",
    "error_occurred = False\n",
    "driver = None\n",
    "limit_of_pages = 0\n",
    "start_page = 0\n",
    "\n",
    "content = \"\"\n",
    "author = \"\"\n",
    "description = \"\"\n",
    "raw_json_content = {}\n",
    "\n",
    "auto_response_count = 0\n",
    "\n",
    "test_specific_url = \"\"\n",
    "\n",
    "def get_response():\n",
    "\treturn input(\"Retry? y/n: \")\n",
    "\n",
    "print(\"[INFO] Loading extracted pages from JSON file...\")\n",
    "with open(\"../archive/pages_extracted/la_cuarta/la_cuarta_240725_cleanData.json\", 'r', encoding='utf-8') as file:\n",
    "\textracted_pages = json.load(file)\n",
    "\tprint(f\"[INFO] {len(extracted_pages['pages'])} pages loaded.\")\n",
    "\tif limit_of_pages == \"all\":\n",
    "\t\tlimit_of_pages = len(extracted_pages[\"pages\"])\n",
    "\t\tprint(f\"[INFO] Limit of pages set to {limit_of_pages}\")\n",
    "\n",
    "def recursive_search(element):\n",
    "\telement_tag_name = element.tag_name\n",
    "\tprint(element_tag_name)\n",
    "\tprint(element.text)\n",
    "\tif element_tag_name == \"p\" or element_tag_name == \"span\" or element_tag_name == \"h2\" or element_tag_name == \"h3\" or element_tag_name == \"h4\" or element_tag_name == \"a\":\n",
    "\t\telement.text\n",
    "\t\treturn element.text.strip()\n",
    "\t\n",
    "\tchildren = element.find_elements(By.XPATH, \".//*\")\n",
    "\tfor child in children:\n",
    "\t\tresult = recursive_search(child)\n",
    "\t\tif result:\n",
    "\t\t\treturn result\n",
    "\treturn \"\"\n",
    "\n",
    "\n",
    "# with open(\"../archive/temp/pages_with_content/la_tercera_with_content_reference.json\", 'r', encoding='utf-8') as file:\n",
    "# \tlast_extracted_pages_with_content = json.load(file)\n",
    "# \tprint(f\"[INFO] {len(last_extracted_pages_with_content['pages'])} pages loaded.\")\n",
    "\n",
    "\n",
    "if start_page >= len(extracted_pages[\"pages\"]):\n",
    "\tprint(f\"[ERROR] start_page is out of range. The maximum page index is {len(extracted_pages['pages']) - 1}.\")\n",
    "\traise SystemExit\n",
    "\n",
    "def extract_from_script():\n",
    "\t\tprint(\"[INFO] Trying to extract content from <script> tags...\")\n",
    "\t\tdata = {\"content\": \"\", \"description\": \"\", \"raw_json_content\": \"\"}\n",
    "\n",
    "\t\tscripts = driver.find_elements(By.XPATH, \"//head/script\")\n",
    "\t\tdata_in_script = {}\n",
    "\t\tcontent_extracted_by_json = False\n",
    "\t\tfor script in scripts: \n",
    "\t\t\tscript_content = script.get_attribute(\"innerHTML\").strip()\n",
    "\n",
    "\t\t\t# Intentar interpretarlo como JSON\n",
    "\t\t\tis_json = False\n",
    "\t\t\tcontent_extracted_by_json = False\n",
    "\t\t\ttry:\n",
    "\t\t\t\t\tdata_in_script = json.loads(script_content)   # si es JSON válido, lo convierte en dict/list\n",
    "\t\t\t\t\ttest = data_in_script[\"articleBody\"]\n",
    "\t\t\t\t\tif test is not None:\n",
    "\t\t\t\t\t\tprint(\"[INFO] Content found in <script> tag.\")\n",
    "\t\t\t\t\tis_json = True\n",
    "\t\t\texcept json.JSONDecodeError:\n",
    "\t\t\t\t\tis_json = False\n",
    "\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tis_json = False\n",
    "\t\t\t\n",
    "\t\t\ttry:\n",
    "\t\t\t\tif is_json:\n",
    "\t\t\t\t\t\tdata[\"content\"] = data_in_script[\"articleBody\"]\n",
    "\t\t\t\t\t\tdata[\"raw_json_content\"] = data_in_script\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\tdata[\"description\"] = data_in_script[\"description\"]\n",
    "\t\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\t\tprint(\"[ERROR] A error occurred while extracting description\", flush=True)\n",
    "\t\t\t\t\t\t\tdata[\"description\"] = \"not found\"\n",
    "\n",
    "\n",
    "\t\t\t\t\t\tcontent_extracted_by_json = True\n",
    "\t\t\t\t\t\tprint(\"[INFO] The content has been extracted. Method: JSON\", flush=True)\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tprint(f\"[ERROR] A error occurred while extracting content from JSON script: {e}\", flush=True)\n",
    "\t\t\t\t\tcontent_extracted_by_json = False\n",
    "\n",
    "\t\treturn content_extracted_by_json, data\n",
    "\n",
    "def extract_from_DOM():\n",
    "\tprint(\"[INFO] Trying to extract content from DOM...\")\n",
    "\tcontent = \"\"\n",
    "\tauthor = \"\"\n",
    "\tdescription = \"\"\n",
    "\n",
    "\t# paragraph = content_container.find_elements(By.TAG_NAME, \"p\") if content_container else None\n",
    "\tparagraph_elements = body.find_elements(By.CLASS_NAME, \"article-body__paragraph\")\n",
    "\tcontent = \"\"\n",
    "\n",
    "\tfor element in paragraph_elements:\n",
    "\t\tp = recursive_search(element)\n",
    "\t\tif \"Volver al Home\" in p or \"También puedes leer\" in p:\n",
    "\t\t\tcontinue\n",
    "\t\tcontent += p + \"\\n\"\n",
    "\n",
    "\ttry:\n",
    "\t\tdescription = body.find_element(By.CLASS_NAME, \"article-head__subtitle\").text.strip()\n",
    "\texcept:\n",
    "\t\tdescription = \"not found\"\n",
    "\n",
    "\tprint(\"[INFO] The content has been extracted. Method: DOM\", flush=True)\n",
    "\tif len(content) < 100:\n",
    "\t\t\tprint(\"[WARNING] The extracted content is too short.\", flush=True)\n",
    "\t\t\tprint(f\"[WARNING] Content: {content}\", flush=True)\n",
    "\treturn content, author, description\n",
    "\n",
    "data_extracted = True\n",
    "re = False\n",
    "auto_re = False\n",
    "\n",
    "def get_driver():\n",
    "\toptions = uc.ChromeOptions()\n",
    "\toptions.add_argument(\"--no-sandbox\")\n",
    "\toptions.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "\tdriver = uc.Chrome(options=options)\n",
    "\tdriver.set_page_load_timeout(15)\n",
    "\treturn driver\n",
    "\n",
    "real_page_index = 0\n",
    "\n",
    "print(\"[INFO] Starting extraction process...\")\n",
    "for page_index, page in enumerate(extracted_pages[\"pages\"]):\n",
    "\n",
    "\t\tif test_specific_url == \"\":\n",
    "\t\t\tif total_pages >= limit_of_pages:\n",
    "\t\t\t\t\tprint(\"[INFO] Limit of pages reached.\")\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\tif \"content\" not in page.keys():\n",
    "\t\t\t\t\tpage[\"content\"] = \"The content has not been extracted yet\"\n",
    "\n",
    "\t\t\tif page_index < start_page:\n",
    "\t\t\t\t\t# if page_index < len(last_extracted_pages_with_content[\"pages\"]):\n",
    "\t\t\t\t\t# \t\textracted_pages_with_content[\"pages\"].append(last_extracted_pages_with_content[\"pages\"][page_index])\n",
    "\t\t\t\t\t# else:\n",
    "\t\t\t\t\t# \textracted_pages_with_content[\"pages\"].append(page)\n",
    "\t\t\t\t\treal_page_index = real_page_index + 1\n",
    "\t\t\t\t\tprint(\"[INFO] Skipping page: \", page_index)\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\tif page[\"content\"] != \"The content has not been extracted yet\" and page[\"content\"] != \"\": \n",
    "\t\t\t\t\tprint(f\"[INFO] Page {page_index} already has content. Skipping...\")\n",
    "\t\t\t\t\textracted_pages_with_content[\"pages\"].append(page)\n",
    "\t\t\t\t\treal_page_index = real_page_index + 1\n",
    "\t\t\t\t\tcontinue\n",
    "\t\telse:\n",
    "\t\t\tprint(\"[INFO] Processing specific page: \", test_specific_url)\n",
    "\t\t\tfor page_aux in extracted_pages[\"pages\"]:\n",
    "\t\t\t\tif page_aux[\"link\"] == test_specific_url:\n",
    "\t\t\t\t\tpage = page_aux\n",
    "\t\t\t\t\ttest_specific_url = \"\"\n",
    "\t\t\t\t\ttotal_pages = 99999\n",
    "\t\t\t\t\tprint(\"[INFO] The specific page found!\")\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\ttotal_pages += 1\n",
    "\n",
    "\t\tprint(f\"\\n[INFO] Checking page: {page_index} | Link: {page['link']}\" )\n",
    "\t\turl = page['link']\n",
    "\t\tfirst = True\n",
    "\t\tauto_re = False\n",
    "\t\tauto_response_count = 0\n",
    "\n",
    "\t\twhile (re or first):\n",
    "\t\t\tfirst = False\n",
    "\t\t\t\n",
    "\t\t\ttry: \n",
    "\t\t\t\ttime.sleep(1)\n",
    "\n",
    "\t\t\t\tif (not re): \n",
    "\t\n",
    "\t\t\t\t\tdriver = get_driver()\n",
    "\t\t\t\t\tprint(\"[INFO] Chrome driver initialized.\")\n",
    "\t\t\t\t\tdriver.get(url)\n",
    "\t\t\t\t\tprint(\"[INFO] Page loaded.\")\n",
    "\n",
    "\t\t\t\tre = False\n",
    "\t\t\t\ttime.sleep(random.uniform(6, 12))\n",
    "\t\t\t\tbody = driver.find_element(By.TAG_NAME, 'body')\n",
    "\n",
    "\t\t\t\ttry: \n",
    "\t\t\t\t\tcontent_extracted_by_json, data = extract_from_script()\n",
    "\t\t\t\t\tif not content_extracted_by_json: \n",
    "\t\t\t\t\t\tcontent, author, description = extract_from_DOM()\n",
    "\n",
    "\t\t\t\t\t\tif author == \"not found\":\n",
    "\t\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\t\tauthor = driver.find_element(By.CLASS_NAME, \"a_md_a\").text.strip()\n",
    "\t\t\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\t\t\tauthor = \"not found\"\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tcontent = data[\"content\"]\n",
    "\t\t\t\t\t\tdescription = data[\"description\"]\n",
    "\t\t\t\t\t\traw_json_content = data[\"raw_json_content\"] if data[\"raw_json_content\"] != \"\" else \"not found initially\"\n",
    "\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\tauthor_meta = driver.find_element(By.CSS_SELECTOR, \"meta[property='article:author']\")\n",
    "\t\t\t\t\t\tauthor = author_meta.get_attribute(\"content\")\n",
    "\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\tprint(\"[ERROR] A error occurred while extracting author name\", flush=True)\n",
    "\t\t\t\t\t\tprint(\"[INFO] Setting author to 'not found'\", flush=True)\n",
    "\t\t\t\t\t\tprint(e, flush=True)\n",
    "\t\t\t\t\t\tauthor = \"not found\"\n",
    "\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tprint(f\"[ERROR] A error occurred while extracting content\", flush=True)\n",
    "\t\t\t\t\traise\n",
    "\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f\"[ERROR] A error occurred while processing the page: {page_index}\", flush=True)\n",
    "\t\t\t\tprint(f\"[ERROR] Site: {url}\", flush=True)\n",
    "\t\t\t\tprint(e, flush=True)\n",
    "\n",
    "\t\t\t\t# response = input(\"Retry? y/n: \")\n",
    "\t\t\t\ttime.sleep(10)\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tif auto_response_count == 3:\n",
    "\t\t\t\t\t\tresponse = \"n\"\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tdriver.refresh()\n",
    "\t\t\t\t\t\ttime.sleep(5)\n",
    "\t\t\t\t\t\tresponse = \"y\"\n",
    "\t\t\t\t\tauto_response_count += 1\n",
    "\n",
    "\t\t\t\t\tif response.lower() == 'y':\n",
    "\t\t\t\t\t\tprint(\"[INFO] Retry...\")\n",
    "\t\t\t\t\t\tre = True\n",
    "\t\t\t\t\telif response.lower() == \"n\" or response == \"\":  \n",
    "\t\t\t\t\t\t# response = input(\"Description? Press enter to not add a description\")\n",
    "\t\t\t\t\t\tresponse = \"[AUTO] Limit of attempts reached\"\n",
    "\t\t\t\t\t\tif not response:\n",
    "\t\t\t\t\t\t\t\tresponse = \"No description provided\"\n",
    "\t\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\t\tprint(f\"[INFO] The user add a description: {response}\")\n",
    "\t\t\t\t\t\tprint(\"[INFO] Cancelling...\")\n",
    "\t\t\t\t\t\terror_occurred = True\n",
    "\t\t\t\t\t\tpage[\"Observation\"] = \"A error occurred while extracting content. The user cancelled the operation. Description: \" + response\n",
    "\t\t\t\t\t\tre = False\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tprint(\"[INFO] The response is unknown.\")\n",
    "\t\t\t\t\t\tprint(\"[INFO] Cancelling...\")\n",
    "\t\t\t\t\t\terror_occurred = True\n",
    "\t\t\t\t\t\tre = False\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tprint(\"[ERROR] a error occurred while processing the page:\", flush=True)\n",
    "\t\t\t\t\tprint(f\"[ERROR] Site: {url}\", flush=True)\n",
    "\t\t\t\t\tprint(e, flush=True)\n",
    "\t\t\t\t\tre = False\n",
    "\t\t\t\t\terror_occurred = True\n",
    "\t\t\t\t\tpage[\"Observation\"] = \"A error occurred while extracting content. Can't retrieve the content.\"\n",
    "\n",
    "\t\tif (not re):\n",
    "\t\t\tdriver.quit()\n",
    "\n",
    "\t\tif error_occurred:\n",
    "\t\t\tcontent = \"A error occurred while extracting content\"\n",
    "\t\t\tauthor = \"A error occurred while extracting content\"\n",
    "\t\t\tdescription = \"A error occurred while extracting content\"\n",
    "\n",
    "\t\t\tpages_failed += 1\n",
    "\t\t\terror_occurred = False\n",
    "\t\telse:\n",
    "\t\t\tpages_succeeded += 1\n",
    "\n",
    "\t\tpage[\"content\"] = content\n",
    "\t\tpage[\"author\"] = author\n",
    "\t\tpage[\"description\"] = description\n",
    "\t\tpage[\"raw_json_content\"] = raw_json_content\n",
    "\t\textracted_pages_with_content[\"pages\"].append(page)\n",
    "\n",
    "\t\tcontent = \"\"\n",
    "\t\tauthor = \"\"\n",
    "\t\tdescription = \"\"\n",
    "\t\traw_json_content = {}\n",
    "\n",
    "\t\tprint(\"[INFO] Storing information in a JSON file...\")\n",
    "\t\twith open(f\"../archive/temp/pages_with_content/la_cuarta/la_cuarta_with_content-{real_page_index}.json\", 'w', encoding='utf-8') as file:\n",
    "\t\t\tjson.dump(extracted_pages_with_content, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\t\treal_page_index = real_page_index + 1\n",
    "\n",
    "\t\textracted_pages_with_content = {\"pages\": []}\n",
    "\n",
    "\t\tdriver.quit()\n",
    "\n",
    "\t\ttime.sleep(random.uniform(6, 12))\n",
    "\n",
    "print(\"Finished processing pages.\")\n",
    "\n",
    "# ----------------- Stats -----------------\n",
    "print(\"\\n\\n\")\n",
    "print(\"----- STATS -----\")\n",
    "\n",
    "try:\n",
    "\tprint(f\"Amount of pages processed: {total_pages}\")\n",
    "\tprint(f\"% Pages succeeded [{pages_succeeded}]: {round(100 * pages_succeeded/total_pages, 2)}\")\n",
    "\tprint(f\"% Pages failed [{pages_failed}]: {round(100 * pages_failed/total_pages, 2)}\")\n",
    "except: \n",
    "\tprint(\"A error occurred while processing the stats\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
