{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import undetected_chromedriver as uc\n",
    "import locale\n",
    "\n",
    "import calendar\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e12debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_pages_info = {\"pages\": []}\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\") \n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "articles_count = 0\n",
    "\n",
    "pages = 5\n",
    "\n",
    "url = f\"https://www.latercera.com/search/?q=Estallido%20Social%20Chile\"\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "for page in range(2, pages + 1):\n",
    "\n",
    "\telements = driver.find_element(By.CLASS_NAME, \"gsc-expansionArea\")\n",
    "\tarticles = elements.find_elements(By.CLASS_NAME, \"gsc-webResult\")\n",
    "\tprint(f\"{len(articles)} articles found\")\n",
    "\n",
    "\tfor article in articles:\n",
    "\t\tprint(\"Analyzing article...\")\n",
    "\n",
    "\t\tdate = article.find_element(By.CLASS_NAME, \"gsc-table-result\").find_element(By.CLASS_NAME, \"gsc-table-cell-snippet-close\").find_element(By.CLASS_NAME, \"gs-bidi-start-align\")\n",
    "\t\tprint(date.text)\n",
    "\t\toriginalDate = date.text[:10] if date else \"not found\"\n",
    "\t\tsplitDate = originalDate.split(\"-\")\n",
    "\n",
    "\t\tif(len(splitDate) != 3 or date == None):\n",
    "\t\t\tprint(\"Fecha no encontrada\")\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\ttimestamp = calendar.timegm(time.strptime(f'{splitDate[2]}-{splitDate[1]}-{splitDate[0]} 00:00:00', '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\t\tif (timestamp < 1573786800 or timestamp > 1702782000):\n",
    "\t\t\tprint(\"La noticia no corresponde a la fecha solicitada\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tprint(splitDate)\n",
    "\n",
    "\t\ttry:\n",
    "\n",
    "\t\t\tlink = article.find_element(By.CLASS_NAME, \"gs-title\").find_element(By.CLASS_NAME, \"gs-title\").get_attribute(\"href\")\n",
    "\t\t\ttitle = article.find_element(By.CLASS_NAME, \"gs-title\").find_element(By.CLASS_NAME, \"gs-title\").text\n",
    "\t\t\timage_link = article.find_element(By.CLASS_NAME, \"gs-image\").find_element(By.CLASS_NAME, \"gs-image\").get_attribute(\"src\")\n",
    "\t\t\tdescription = article.find_element(By.CLASS_NAME, \"gsc-table-result\").find_element(By.CLASS_NAME, \"gsc-table-cell-snippet-close\").find_element(By.CLASS_NAME, \"gs-bidi-start-align\")\n",
    "\t\t\tdescription = description.text[10:] if description else \"not found\"\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(\"Error al obtener los datos del artículo: \", e)\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tlink_info = {\n",
    "\t\t\t\"newscast\" : \"La Tercera\",\n",
    "\t\t\t\"title\": title,\n",
    "\t\t\t\"description\": description,\n",
    "\t\t\t\"category\": \"The site does not provide a category\",\n",
    "\t\t\t\"date\": originalDate,\n",
    "\t\t\t\"image_link\": image_link,\n",
    "\t\t\t\"author\": \"not found initially\",\n",
    "\t\t\t\"link\": link,\n",
    "\t\t}\n",
    "\t\tarticles_count += 1\n",
    "\t\tprint(\"Añadiendo articulo\")\n",
    "\t\tjson_pages_info[\"pages\"].append(link_info)\n",
    "\n",
    "\tbuttons = driver.find_element(By.CLASS_NAME, \"gsc-cursor\")\n",
    "\tbuttons = buttons.find_elements(By.CLASS_NAME, \"gsc-cursor-page\")\n",
    "\tprint(len(buttons))\n",
    "\n",
    "\tbutton_found = False\n",
    "\n",
    "\tfor button in buttons:\n",
    "\t\tif(button.text == str(page)):\n",
    "\t\t\tprint(\"Button found: \" + button.text)\n",
    "\t\t\tbutton.click()\n",
    "\t\t\tbutton_found = True\n",
    "\t\t\ttime.sleep(2)\n",
    "\t\t\tbreak\n",
    "\n",
    "\tif(not button_found):\n",
    "\t\t\tprint(\"The button for page \" + str(page) + \" was not found.\")\n",
    "\t\t\tbreak\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "with open(f\"../archive/temp/laTercera_pages.json\", 'w', encoding='utf-8') as file:\n",
    "\tjson.dump(json_pages_info, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d76b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import random\n",
    "\n",
    "months = {\n",
    "    \"ene\": \"Jan\", \"feb\": \"Feb\", \"mar\": \"Mar\", \"abr\": \"Apr\",\n",
    "    \"may\": \"May\", \"jun\": \"Jun\", \"jul\": \"Jul\", \"ago\": \"Aug\",\n",
    "    \"sept\": \"Sep\", \"oct\": \"Oct\", \"nov\": \"Nov\", \"dic\": \"Dec\"\n",
    "}\n",
    "\n",
    "locale.setlocale(locale.LC_TIME, 'C')\n",
    "\n",
    "# json_pages_info = {\"pages\": []}\n",
    "\n",
    "pages = 32\n",
    "\n",
    "data_extracted = True\n",
    "\n",
    "re = False\n",
    "\n",
    "for page in range(22, pages):\n",
    "\n",
    "\tfirst = True\n",
    "\twhile (re or first):\n",
    "\t\tfirst = False\n",
    "\t\t\n",
    "\t\ttry: \n",
    "\t\t\ttime.sleep(1)\n",
    "\n",
    "\t\t\tif (not re): \n",
    "\t\t\t\toptions = uc.ChromeOptions()\n",
    "\t\t\t\toptions.add_argument(\"--no-sandbox\")\n",
    "\t\t\t\toptions.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "\t\t\t\tdriver = uc.Chrome(options=options)\n",
    "\n",
    "\t\t\t\turl = f'https://www.google.com/search?q=%22Estallido+social%22+site%3Awww.latercera.com&tbs=cdr:1,cd_min:11/15/2019,cd_max:12/17/2023&start={page * 10}'\n",
    "\t\t\t\tdriver.get(url)\n",
    "\n",
    "\t\t\tre = False\n",
    "\t\t\ttime.sleep(random.uniform(6, 12))\n",
    "\n",
    "\t\t\tarticles_section = driver.find_element(By.CLASS_NAME, \"dURPMd\")\n",
    "\t\t\tarticles = articles_section.find_elements(By.CLASS_NAME, \"MjjYud\")\n",
    "\n",
    "\t\t\tlocation = articles_section.location\n",
    "\t\t\tsize = articles_section.size\n",
    "\t\t\tx = location['x'] + random.randint(5, size['width'] - 5)\n",
    "\t\t\ty = location['y'] + random.randint(5, size['height'] - 5)\n",
    "\t\t\tduration = random.uniform(0.8, 2.0)\n",
    "\t\t\tpyautogui.moveTo(x, y, duration=duration)\n",
    "\n",
    "\t\t\tfor article in articles:\n",
    "\t\t\t\tprint(\"revisando artículo...\")\n",
    "\t\t\t\ttry: \n",
    "\t\t\t\t\toriginalDate = article.find_element(By.CLASS_NAME, \"YrbPuc\").find_element(By.TAG_NAME, \"span\").text\n",
    "\t\t\t\t\tfor es, en in months.items():\n",
    "\t\t\t\t\t\tif es in originalDate:\n",
    "\t\t\t\t\t\t\t\toriginalDate = originalDate.replace(es, en)\n",
    "\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\tdate_epoch = int(time.mktime(time.strptime(originalDate, \"%d %b %Y\")))\n",
    "\n",
    "\t\t\t\t\tif (date_epoch < 1573786800 or date_epoch > 1702782000):\n",
    "\t\t\t\t\t\tprint(\"La noticia no corresponde a la fecha solicitada\")\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\ttitle = article.find_element(By.TAG_NAME, \"h3\").text\n",
    "\t\t\t\t\t\tdescription = article.find_element(By.CLASS_NAME, \"kb0PBd \").find_elements(By.TAG_NAME, \"span\")[1].text\n",
    "\t\t\t\t\t\tlink = article.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "\n",
    "\t\t\t\t\t\tlink_info = {\n",
    "\t\t\t\t\t\t\t\"newscast\" : \"El Mostrador\",\n",
    "\t\t\t\t\t\t\t\"title\": title,\n",
    "\t\t\t\t\t\t\t\"description\": description,\n",
    "\t\t\t\t\t\t\t\"category\": \"The site does not provide a category\",\n",
    "\t\t\t\t\t\t\t\"date\": originalDate,\n",
    "\t\t\t\t\t\t\t\"image_link\": \"not found initially\",\n",
    "\t\t\t\t\t\t\t\"author\": \"not found initially\",\n",
    "\t\t\t\t\t\t\t\"link\": link,\n",
    "\t\t\t\t\t\t}\n",
    "\n",
    "\t\t\t\t\t\tprint(\"información incluida!\")\n",
    "\t\t\t\t\t\tjson_pages_info[\"pages\"].append(link_info)\n",
    "\n",
    "\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\tprint(f\"Error al extraer información del artículo: {e}\")\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tprint(f\"Error al procesar el artículo: {e}\")\n",
    "\t\t\t\t\tcontinue\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error al procesar el resultado de busqueda. Página: {page}\")\n",
    "\t\t\tprint(f\"Sitio: {url}\")\n",
    "\t\t\tprint(e)\n",
    "\t\t\tresponse = input(\"Reintentar? y/n\")\n",
    "\t\t\tif response.lower() == 'y':\n",
    "\t\t\t\tprint(\"Reintentando.\")\n",
    "\t\t\t\tre = True\n",
    "\t\t\telse: \n",
    "\t\t\t\tprint(\"Cancelando...\")\n",
    "\t\t\t\tre = False\n",
    "\t\t\n",
    "\t\tif (not re):\n",
    "\t\t\tdriver.quit()\n",
    "\n",
    "print(\"Almacenando información en el archivo JSON...\")\n",
    "with open(f\"../archive/temp/la_tercera_080725.json\", 'w', encoding='utf-8') as file:\n",
    "\tjson.dump(json_pages_info, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "# with open(f\"output.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "# \tf.write(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace4fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "all_data = {\"pages\": []}\n",
    "coincidence = 0\n",
    "clean_data_links = []\n",
    "\n",
    "clean_data = {\n",
    "\t\"pages\": []\n",
    "}\n",
    "\n",
    "with open(\"../archive/temp/la_tercera_090725.json\", 'r', encoding='utf-8') as file:\n",
    "\tall_data = json.load(file)\n",
    "\n",
    "all_links = [page[\"link\"] for page in all_data[\"pages\"]]\n",
    "\n",
    "coincidence_boolean = False\n",
    "\n",
    "print(f\"Total links in raw data: {len(all_data['pages'])}\")\n",
    "\n",
    "for page in all_data[\"pages\"]:\n",
    "\tfor link in clean_data_links:\n",
    "\t\tif page[\"link\"] in link:\n",
    "\t\t\tprint(f\"Coincidencia encontrada: {page['link']} con link {link}\")\n",
    "\t\t\tcoincidence += 1\n",
    "\t\t\tcoincidence_boolean = True\n",
    "\t\t\tbreak\n",
    "\tif not coincidence_boolean:\n",
    "\t\tclean_data[\"pages\"].append(page)\n",
    "\t\tclean_data_links.append(page[\"link\"])\n",
    "\tcoincidence_boolean = False\n",
    "\n",
    "print(f\"Total coincidences: {coincidence}\")\n",
    "print(f\"Total unique links: {len(clean_data['pages'])}\")\n",
    "\n",
    "with open(f\"../archive/temp/la_tercera_090725_cleanData.json\", 'w', encoding='utf-8') as file:\n",
    "\tjson.dump(clean_data, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8bd0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {}\n",
    "clean_data = {\"pages\": []}\n",
    "\n",
    "with open(\"../archive/temp/la_tercera_080725_raw.json\", 'r', encoding='utf-8') as file:\n",
    "\tdata = json.load(file)\n",
    "\n",
    "for page in data[\"pages\"]:\n",
    "\tpage[\"newscast\"] = \"La Tercera\"\n",
    "\tclean_data[\"pages\"].append(page)\n",
    "\n",
    "with open(f\"../archive/temp/la_tercera_090725.json\", 'w', encoding='utf-8') as file:\n",
    "\tjson.dump(clean_data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a0c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "extracted_pages = {\"pages\": []}\n",
    "extracted_pages_with_content = {\"pages\": []}\n",
    "\n",
    "with open(\"../archive/pages_extracted/la_tercera/la_tercera_090725_cleanData.json\", 'r', encoding='utf-8') as file:\n",
    "\textracted_pages = json.load(file)\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\") \n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--remote-debugging-port=9222\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--disable-software-rasterizer\")\n",
    "options.add_argument(\"--enable-features=NetworkService,NetworkServiceInProcess\")\n",
    "options.add_argument(\"--browser-version=139\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "data_extracted = True\n",
    "re = False\n",
    "\n",
    "for page_index, page in enumerate(extracted_pages[\"pages\"]):\n",
    "\tprint(f\"Link - {page['link']}\" )\n",
    "\n",
    "\turl = page['link']\n",
    "\n",
    "\tfirst = True\n",
    "\n",
    "\twhile (re or first):\n",
    "\t\tfirst = False\n",
    "\t\t\n",
    "\t\ttry: \n",
    "\t\t\ttime.sleep(1)\n",
    "\n",
    "\t\t\tif (not re): \n",
    "\t\t\t\toptions = uc.ChromeOptions()\n",
    "\t\t\t\toptions.add_argument(\"--no-sandbox\")\n",
    "\t\t\t\toptions.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "\t\t\t\tdriver = uc.Chrome(options=options)\n",
    "\n",
    "\t\t\t\tdriver.get(url)\n",
    "\n",
    "\t\t\tre = False\n",
    "\t\t\ttime.sleep(random.uniform(6, 12))\n",
    "\n",
    "\t\t\ttry: \n",
    "\t\t\t\ttitle = driver.find_element(By.TAG_NAME, \"h1\").text\n",
    "\t\t\t\textracted_pages_with_content[\"pages\"].append({\"title\": title, \"page\": page})\n",
    "\t\t\t\tprint(title)\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f\"Error al procesar el artículo: {e}\")\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error al procesar. Página: {page_index}\")\n",
    "\t\t\tprint(f\"Sitio: {url}\")\n",
    "\t\t\tprint(e)\n",
    "\t\t\tresponse = input(\"Reintentar? y/n\")\n",
    "\t\t\tif response.lower() == 'y':\n",
    "\t\t\t\tprint(\"Reintentando.\")\n",
    "\t\t\t\tre = True\n",
    "\t\t\telse: \n",
    "\t\t\t\tprint(\"Cancelando...\")\n",
    "\t\t\t\tre = False\n",
    "\t\t\n",
    "\t\tif (not re):\n",
    "\t\t\tdriver.quit()\n",
    "\n",
    "\tprint(\"Almacenando información en el archivo JSON...\")\n",
    "\twith open(f\"../archive/temp/ciper_pages_with_content.json\", 'w', encoding='utf-8') as file:\n",
    "\t\tjson.dump(extracted_pages_with_content, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\ttime.sleep(random.uniform(6, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd20fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extracted_pages = {\"pages\": []}\n",
    "extracted_pages_with_content = {\"pages\": []}\n",
    "last_extracted_pages_with_content = {\"pages\": []}\n",
    "\n",
    "total_pages = 0\n",
    "pages_succeeded = 0\n",
    "pages_failed = 0\n",
    "error_occurred = False\n",
    "driver = None\n",
    "limit_of_pages = \"all\"\n",
    "start_page = 0\n",
    "\n",
    "content = \"\"\n",
    "author = \"\"\n",
    "description = \"\"\n",
    "raw_json_content = {}\n",
    "\n",
    "auto_response_count = 0\n",
    "\n",
    "test_specific_url = \"\"\n",
    "\n",
    "def get_response():\n",
    "\treturn input(\"Retry? y/n: \")\n",
    "\n",
    "print(\"[INFO] Loading extracted pages from JSON file...\")\n",
    "with open(\"../archive/temp/pages_with_content/la_tercera/la_tercera_pages_to_check.json\", 'r', encoding='utf-8') as file:\n",
    "\textracted_pages = json.load(file)\n",
    "\tprint(f\"[INFO] {len(extracted_pages['pages'])} pages loaded.\")\n",
    "\tif limit_of_pages == \"all\":\n",
    "\t\tlimit_of_pages = len(extracted_pages[\"pages\"])\n",
    "\t\tprint(f\"[INFO] Limit of pages set to {limit_of_pages}\")\n",
    "\n",
    "def recursive_search(element):\n",
    "\telement_tag_name = element.tag_name\n",
    "\tprint(element_tag_name)\n",
    "\tprint(element.text)\n",
    "\tif element_tag_name == \"p\" or element_tag_name == \"span\" or element_tag_name == \"h2\" or element_tag_name == \"h3\" or element_tag_name == \"h4\" or element_tag_name == \"a\":\n",
    "\t\telement.text\n",
    "\t\treturn element.text.strip()\n",
    "\t\n",
    "\tchildren = element.find_elements(By.XPATH, \".//*\")\n",
    "\tfor child in children:\n",
    "\t\tresult = recursive_search(child)\n",
    "\t\tif result:\n",
    "\t\t\treturn result\n",
    "\treturn \"\"\n",
    "\n",
    "\n",
    "# with open(\"../archive/temp/pages_with_content/la_tercera_with_content_reference.json\", 'r', encoding='utf-8') as file:\n",
    "# \tlast_extracted_pages_with_content = json.load(file)\n",
    "# \tprint(f\"[INFO] {len(last_extracted_pages_with_content['pages'])} pages loaded.\")\n",
    "\n",
    "\n",
    "if start_page >= len(extracted_pages[\"pages\"]):\n",
    "\tprint(f\"[ERROR] start_page is out of range. The maximum page index is {len(extracted_pages['pages']) - 1}.\")\n",
    "\traise SystemExit\n",
    "\n",
    "def extract_from_script():\n",
    "\t\tprint(\"[INFO] Trying to extract content from <script> tags...\")\n",
    "\t\tdata = {\"content\": \"\", \"description\": \"\", \"raw_json_content\": \"\"}\n",
    "\n",
    "\t\tscripts = driver.find_elements(By.XPATH, \"//head/script\")\n",
    "\t\tdata_in_script = {}\n",
    "\t\tcontent_extracted_by_json = False\n",
    "\t\tfor script in scripts: \n",
    "\t\t\tscript_content = script.get_attribute(\"innerHTML\").strip()\n",
    "\n",
    "\t\t\t# Intentar interpretarlo como JSON\n",
    "\t\t\tis_json = False\n",
    "\t\t\tcontent_extracted_by_json = False\n",
    "\t\t\ttry:\n",
    "\t\t\t\t\tdata_in_script = json.loads(script_content)   # si es JSON válido, lo convierte en dict/list\n",
    "\t\t\t\t\ttest = data_in_script[\"articleBody\"]\n",
    "\t\t\t\t\tif test is not None:\n",
    "\t\t\t\t\t\tprint(\"[INFO] Content found in <script> tag.\")\n",
    "\t\t\t\t\tis_json = True\n",
    "\t\t\texcept json.JSONDecodeError:\n",
    "\t\t\t\t\tis_json = False\n",
    "\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tis_json = False\n",
    "\t\t\t\n",
    "\t\t\ttry:\n",
    "\t\t\t\tif is_json:\n",
    "\t\t\t\t\t\tdata[\"content\"] = data_in_script[\"articleBody\"]\n",
    "\t\t\t\t\t\tdata[\"raw_json_content\"] = data_in_script\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\tdata[\"description\"] = data_in_script[\"description\"]\n",
    "\t\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\t\tprint(\"[ERROR] A error occurred while extracting description\", flush=True)\n",
    "\t\t\t\t\t\t\tdata[\"description\"] = \"not found\"\n",
    "\n",
    "\n",
    "\t\t\t\t\t\tcontent_extracted_by_json = True\n",
    "\t\t\t\t\t\tprint(\"[INFO] The content has been extracted. Method: JSON\", flush=True)\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tprint(f\"[ERROR] A error occurred while extracting content from JSON script: {e}\", flush=True)\n",
    "\t\t\t\t\tcontent_extracted_by_json = False\n",
    "\n",
    "\t\treturn content_extracted_by_json, data\n",
    "\n",
    "def extract_from_DOM():\n",
    "\tprint(\"[INFO] Trying to extract content from DOM...\")\n",
    "\tcontent = \"\"\n",
    "\tauthor = \"\"\n",
    "\tdescription = \"\"\n",
    "\n",
    "\t# paragraph = content_container.find_elements(By.TAG_NAME, \"p\") if content_container else None\n",
    "\tparagraph_elements = body.find_elements(By.CLASS_NAME, \"article-body__paragraph\")\n",
    "\tcontent = \"\"\n",
    "\n",
    "\tfor element in paragraph_elements:\n",
    "\t\tp = recursive_search(element)\n",
    "\t\tif \"Volver al Home\" in p or \"También puedes leer\" in p:\n",
    "\t\t\tcontinue\n",
    "\t\tcontent += p + \"\\n\"\n",
    "\n",
    "\ttry:\n",
    "\t\tdescription = body.find_element(By.CLASS_NAME, \"article-head__subtitle\").text.strip()\n",
    "\texcept:\n",
    "\t\tdescription = \"not found\"\n",
    "\n",
    "\tprint(\"[INFO] The content has been extracted. Method: DOM\", flush=True)\n",
    "\tif len(content) < 100:\n",
    "\t\t\tprint(\"[WARNING] The extracted content is too short.\", flush=True)\n",
    "\t\t\tprint(f\"[WARNING] Content: {content}\", flush=True)\n",
    "\treturn content, author, description\n",
    "\n",
    "data_extracted = True\n",
    "re = False\n",
    "auto_re = False\n",
    "\n",
    "def get_driver():\n",
    "\toptions = uc.ChromeOptions()\n",
    "\toptions.add_argument(\"--no-sandbox\")\n",
    "\toptions.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "\tdriver = uc.Chrome(options=options)\n",
    "\tdriver.set_page_load_timeout(15)\n",
    "\treturn driver\n",
    "\n",
    "print(\"[INFO] Starting extraction process...\")\n",
    "for page_index, page in enumerate(extracted_pages[\"pages\"]):\n",
    "\n",
    "\t\tif test_specific_url == \"\":\n",
    "\t\t\tif total_pages >= limit_of_pages:\n",
    "\t\t\t\t\tprint(\"[INFO] Limit of pages reached.\")\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\tif \"content\" not in page.keys():\n",
    "\t\t\t\t\tpage[\"content\"] = \"The content has not been extracted yet\"\n",
    "\n",
    "\t\t\tif page_index < start_page:\n",
    "\t\t\t\t\t# if page_index < len(last_extracted_pages_with_content[\"pages\"]):\n",
    "\t\t\t\t\t# \t\textracted_pages_with_content[\"pages\"].append(last_extracted_pages_with_content[\"pages\"][page_index])\n",
    "\t\t\t\t\t# else:\n",
    "\t\t\t\t\t# \textracted_pages_with_content[\"pages\"].append(page)\n",
    "\t\t\t\t\tprint(\"[INFO] Skipping page: \", page_index)\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\tif page[\"content\"] != \"The content has not been extracted yet\" and page[\"content\"] != \"\": \n",
    "\t\t\t\t\tprint(f\"[INFO] Page {page_index} already has content. Skipping...\")\n",
    "\t\t\t\t\textracted_pages_with_content[\"pages\"].append(page)\n",
    "\t\t\t\t\tcontinue\n",
    "\t\telse:\n",
    "\t\t\tprint(\"[INFO] Processing specific page: \", test_specific_url)\n",
    "\t\t\tfor page_aux in extracted_pages[\"pages\"]:\n",
    "\t\t\t\tif page_aux[\"link\"] == test_specific_url:\n",
    "\t\t\t\t\tpage = page_aux\n",
    "\t\t\t\t\ttest_specific_url = \"\"\n",
    "\t\t\t\t\ttotal_pages = 99999\n",
    "\t\t\t\t\tprint(\"[INFO] The specific page found!\")\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\ttotal_pages += 1\n",
    "\n",
    "\t\tprint(f\"\\n[INFO] Checking page: {page_index} | Link: {page['link']}\" )\n",
    "\t\turl = page['link']\n",
    "\t\tfirst = True\n",
    "\t\tauto_re = False\n",
    "\t\tauto_response_count = 0\n",
    "\n",
    "\t\twhile (re or first):\n",
    "\t\t\tfirst = False\n",
    "\t\t\t\n",
    "\t\t\ttry: \n",
    "\t\t\t\ttime.sleep(1)\n",
    "\n",
    "\t\t\t\tif (not re): \n",
    "\t\n",
    "\t\t\t\t\tdriver = get_driver()\n",
    "\t\t\t\t\tprint(\"[INFO] Chrome driver initialized.\")\n",
    "\t\t\t\t\tdriver.get(url)\n",
    "\t\t\t\t\tprint(\"[INFO] Page loaded.\")\n",
    "\n",
    "\t\t\t\tre = False\n",
    "\t\t\t\ttime.sleep(random.uniform(6, 12))\n",
    "\t\t\t\tbody = driver.find_element(By.TAG_NAME, 'body')\n",
    "\n",
    "\t\t\t\ttry: \n",
    "\t\t\t\t\tcontent_extracted_by_json, data = extract_from_script()\n",
    "\t\t\t\t\tif not content_extracted_by_json: \n",
    "\t\t\t\t\t\tcontent, author, description = extract_from_DOM()\n",
    "\n",
    "\t\t\t\t\t\tif author == \"not found\":\n",
    "\t\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\t\tauthor = driver.find_element(By.CLASS_NAME, \"a_md_a\").text.strip()\n",
    "\t\t\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\t\t\tauthor = \"not found\"\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tcontent = data[\"content\"]\n",
    "\t\t\t\t\t\tdescription = data[\"description\"]\n",
    "\t\t\t\t\t\traw_json_content = data[\"raw_json_content\"] if data[\"raw_json_content\"] != \"\" else \"not found initially\"\n",
    "\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\tauthor_meta = driver.find_element(By.CSS_SELECTOR, \"meta[property='article:author']\")\n",
    "\t\t\t\t\t\tauthor = author_meta.get_attribute(\"content\")\n",
    "\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\tprint(\"[ERROR] A error occurred while extracting author name\", flush=True)\n",
    "\t\t\t\t\t\tprint(\"[INFO] Setting author to 'not found'\", flush=True)\n",
    "\t\t\t\t\t\tprint(e, flush=True)\n",
    "\t\t\t\t\t\tauthor = \"not found\"\n",
    "\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tprint(f\"[ERROR] A error occurred while extracting content\", flush=True)\n",
    "\t\t\t\t\traise\n",
    "\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f\"[ERROR] A error occurred while processing the page: {page_index}\", flush=True)\n",
    "\t\t\t\tprint(f\"[ERROR] Site: {url}\", flush=True)\n",
    "\t\t\t\tprint(e, flush=True)\n",
    "\n",
    "\t\t\t\t# response = input(\"Retry? y/n: \")\n",
    "\t\t\t\ttime.sleep(10)\n",
    "\n",
    "\t\t\t\tif auto_response_count == 3:\n",
    "\t\t\t\t\tresponse = \"n\"\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tresponse = \"y\"\n",
    "\t\t\t\tauto_response_count += 1\n",
    "\n",
    "\t\t\t\tif response.lower() == 'y':\n",
    "\t\t\t\t\tprint(\"[INFO] Retry...\")\n",
    "\t\t\t\t\tre = True\n",
    "\t\t\t\telif response.lower() == \"n\" or response == \"\":  \n",
    "\t\t\t\t\t# response = input(\"Description? Press enter to not add a description\")\n",
    "\t\t\t\t\tresponse = \"[AUTO] Limit of attempts reached\"\n",
    "\t\t\t\t\tif not response:\n",
    "\t\t\t\t\t\t\tresponse = \"No description provided\"\n",
    "\t\t\t\t\telse: \n",
    "\t\t\t\t\t\t\tprint(f\"[INFO] The user add a description: {response}\")\n",
    "\t\t\t\t\tprint(\"[INFO] Cancelling...\")\n",
    "\t\t\t\t\terror_occurred = True\n",
    "\t\t\t\t\tpage[\"Observation\"] = \"A error occurred while extracting content. The user cancelled the operation. Description: \" + response\n",
    "\t\t\t\t\tre = False\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(\"[INFO] The response is unknown.\")\n",
    "\t\t\t\t\tprint(\"[INFO] Cancelling...\")\n",
    "\t\t\t\t\terror_occurred = True\n",
    "\t\t\t\t\tre = False\n",
    "\n",
    "\t\tif (not re):\n",
    "\t\t\tdriver.quit()\n",
    "\n",
    "\t\tif error_occurred:\n",
    "\t\t\tcontent = \"A error occurred while extracting content\"\n",
    "\t\t\tauthor = \"A error occurred while extracting content\"\n",
    "\t\t\tdescription = \"A error occurred while extracting content\"\n",
    "\n",
    "\t\t\tpages_failed += 1\n",
    "\t\t\terror_occurred = False\n",
    "\t\telse:\n",
    "\t\t\tpages_succeeded += 1\n",
    "\n",
    "\t\tpage[\"content\"] = content\n",
    "\t\tpage[\"author\"] = author\n",
    "\t\tpage[\"description\"] = description\n",
    "\t\tpage[\"raw_json_content\"] = raw_json_content\n",
    "\t\textracted_pages_with_content[\"pages\"].append(page)\n",
    "\n",
    "\t\tcontent = \"\"\n",
    "\t\tauthor = \"\"\n",
    "\t\tdescription = \"\"\n",
    "\t\traw_json_content = {}\n",
    "\n",
    "\t\tprint(\"[INFO] Storing information in a JSON file...\")\n",
    "\t\twith open(f\"../archive/temp/pages_with_content/la_tercera/la_tercera_with_content-{page_index}.json\", 'w', encoding='utf-8') as file:\n",
    "\t\t\tjson.dump(extracted_pages_with_content, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\t\textracted_pages_with_content = {\"pages\": []}\n",
    "\n",
    "\t\tdriver.quit()\n",
    "\n",
    "\t\ttime.sleep(random.uniform(6, 12))\n",
    "\n",
    "# ----------------- Stats -----------------\n",
    "print(\"\\n\\n\")\n",
    "print(\"----- STATS -----\")\n",
    "\n",
    "try:\n",
    "\tprint(f\"Amount of pages processed: {total_pages}\")\n",
    "\tprint(f\"% Pages succeeded [{pages_succeeded}]: {round(100 * pages_succeeded/total_pages, 2)}\")\n",
    "\tprint(f\"% Pages failed [{pages_failed}]: {round(100 * pages_failed/total_pages, 2)}\")\n",
    "except: \n",
    "\tprint(\"A error occurred while processing the stats\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
