{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch News articles from November 15, 2019 to December 17, 2023.\n",
    "\n",
    "The data is extracted by looking for the following websites:\n",
    "\n",
    "- El Pais\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. El País\n",
    "## 1.1. Extract Pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_data(data):\n",
    "\ttry:\n",
    "\t\tdata = data.text\n",
    "\t\treturn data\n",
    "\texcept:\n",
    "\t\treturn \"None\"\n",
    "\n",
    "def get_pages(term, document_path):\n",
    "\n",
    "\tsearch_term = term\n",
    "\tsearch_term = search_term.replace(' ', '+')  \n",
    "\tnum_pages = 1\n",
    "\tjson_pages_info = {\n",
    "\t\t\"pages\": []\n",
    "\t}\n",
    "\n",
    "\theaders = {'User-Agent': 'Chrome/58.0.3029.110'}\n",
    "\n",
    "\tall_pages_links = []\n",
    "\n",
    "\tfor page in range(0, num_pages):\n",
    "\t\turl = f'https://elpais.com/buscador/Estallido%20Social%20en%20Chile/{page}'\n",
    "\t\tresponse = requests.get(url, headers=headers)\n",
    "\n",
    "\t\tif response.status_code == 200:\n",
    "\t\t\tprint(\"status 200\")\n",
    "\n",
    "\t\t\tresponse.encoding = 'ISO-8859-1'\n",
    "\t\t\tcontent = response.content.decode('ISO-8859-1')\n",
    "\t\t\tsoup = BeautifulSoup(content, 'html.parser')\n",
    "\t\t\t\n",
    "\t\t\twith open('output.html', 'w', encoding='utf-8') as f:\n",
    "\t\t\t\tf.write(soup.prettify())\n",
    "\n",
    "\t\t\tnews = soup.find(\"div\", class_=\"bu_b _g-o\").find_all('article')\n",
    "\n",
    "\t\t\tfor page_section in news:\n",
    "\n",
    "\t\t\t\ttimestamp = (int)(page_section[\"data-timestamp\"])\n",
    "\t\t\t\tif (timestamp < 1573786800 or timestamp > 1702782000):\n",
    "\t\t\t\t\tprint(\"La noticia no corresponde a la fecha solicitada\")\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\ttitle = verify_data(page_section.find(\"header\").find(\"h2\"))\n",
    "\t\t\t\tcategory = verify_data(page_section.find(\"header\").find(\"a\"))\n",
    "\t\t\t\tdescription = verify_data(page_section.find(\"p\", class_=\"c_d\"))\n",
    "\t\t\t\tdate = verify_data(page_section.find(\"div\", class_=\"c_f\"))\n",
    "\t\t\t\timage_link = page_section.find(\"figure\").find(\"a\", href=True)[\"href\"]\n",
    "\t\t\t\tauthor = verify_data(page_section.find(\"div\", class_=\"c_a\"))\n",
    "\n",
    "\t\t\t\tlink_section = page_section.find(\"header\").find(\"h2\").find('a', href=True)\n",
    "\t\t\t\tlink = link_section['href']\n",
    "\n",
    "\t\t\t\tif (all_pages_links.count(link) > 0):\n",
    "\t\t\t\t\tprint(\"Sitio ya ingresado\")\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tall_pages_links.append(link)\n",
    "\n",
    "\t\t\t\tlink_info = {\n",
    "\t\t\t\t\t\"newscast\" : \"El País\",\n",
    "\t\t\t\t\t\"title\": title,\n",
    "\t\t\t\t\t\"description\": description,\n",
    "\t\t\t\t\t\"category\": category,\n",
    "\t\t\t\t\t\"date\": date,\n",
    "\t\t\t\t\t\"image_link\": image_link,\n",
    "\t\t\t\t\t\"author\": author,\n",
    "\t\t\t\t\t\"link\": link,\n",
    "\t\t\t\t}\n",
    "\t\t\t\tjson_pages_info[\"pages\"].append(link_info)\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"Error al acceder a la página {page + 1}: {response.status_code}\")\n",
    "\t\t\t\n",
    "\tprint(f\"Se han encontrado: {len(json_pages_info['pages'])} páginas\")\n",
    "\n",
    "\twith open(f\"{document_path}\", 'w', encoding='utf-8') as file:\n",
    "\t\tjson.dump(json_pages_info, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pages('Estallido Social Chile', './pages_extracted/elpais_pages.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 El Pais - Extract content from each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_content(page, url):\n",
    "\n",
    "\tpage[\"page_content\"] = {}\n",
    "\n",
    "\theaders = {'User-Agent': 'Chrome/58.0.3029.110'}\n",
    "\n",
    "\tresponse = requests.get(url, headers=headers)\n",
    "\n",
    "\tif response.status_code == 200:\n",
    "\n",
    "\t\tpage_content = {}\n",
    "\t\tprint(\"status 200\")\n",
    "\n",
    "\t\tresponse.encoding = 'utf-8'\n",
    "\t\tcontent = response.content.decode('utf-8')\n",
    "\t\tsoup = BeautifulSoup(content, 'html.parser')\n",
    "\t\t\n",
    "\t\twith open('output.html', 'w', encoding='utf-8') as f:\n",
    "\t\t\tf.write(soup.prettify())\n",
    "\n",
    "\t\tprint(\"Extracción de datos...\")\n",
    "\n",
    "\t\ttext = \"\"\n",
    "\n",
    "\t\ttitle = soup.find(\"h1\", class_=\"a_t\")\n",
    "\t\tcontent = soup.find(\"div\", class_=\"a_c clearfix\")\n",
    "\t\tparagraph = content.find_all(\"p\") if content else None\n",
    "\t\tfor p in paragraph:\n",
    "\t\t\ttext += p.text.strip() + \"\\n\"\n",
    "\n",
    "\t\tpage_content = {\n",
    "\t\t\t\"header\": title.text.strip() if title else \"No header found\",\n",
    "\t\t\t\"content\": text.strip() if text else \"No content found\",\n",
    "\t\t}\n",
    "\n",
    "\t\tpage[\"page_content\"] = page_content\n",
    "\telse:\n",
    "\t\tprint(f\"Error al acceder al contenido de la página {url}: {response.status_code}\")\n",
    "\t\t\t\n",
    "\t# print(f\"Se han encontrado: {len(json_data['content'])} páginas\")\n",
    "\n",
    "\t# with open(f\"{document_path}\", 'w', encoding='utf-8') as file:\n",
    "\t# \tjson.dump(json_data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_json_data = {}\n",
    "# data_decoded = {\"pages\": []}\n",
    "\n",
    "# with open(\"pages_extracted/elpais_pages.json\", 'r', encoding='latin1') as file:\n",
    "#     original_json_data = json.load(file)\n",
    "#     print(len(original_json_data[\"pages\"]))\n",
    "\n",
    "# for page in original_json_data[\"pages\"]:\n",
    "# \tpage[\"newscast\"] = page[\"newscast\"].encode(\"latin1\").decode(\"utf-8\")\n",
    "# \tpage[\"title\"] = page[\"title\"].encode(\"latin1\").decode(\"utf-8\")\n",
    "# \tpage[\"description\"] = page[\"description\"].encode(\"latin1\").decode(\"utf-8\")\n",
    "# \tpage[\"category\"] = page[\"category\"].encode(\"latin1\").decode(\"utf-8\")\n",
    "# \tpage[\"date\"] = page[\"date\"].encode(\"latin1\").decode(\"utf-8\")\n",
    "# \tpage[\"image_link\"] = page[\"image_link\"].encode(\"latin1\").decode(\"utf-8\")\n",
    "# \tpage[\"author\"] = page[\"author\"].encode(\"latin1\").decode(\"utf-8\")\n",
    "# \tpage[\"link\"] = page[\"link\"].encode(\"latin1\").decode(\"utf-8\")\n",
    "# \tdata_decoded[\"pages\"].append(page)\n",
    "      \n",
    "# with open(\"data/test.json\", 'w', encoding='utf-8') as file:\n",
    "# \tjson.dump(data_decoded, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_json_data = {}\n",
    "json_data_with_content ={\"pages\": []}\n",
    "\n",
    "with open(\"pages_extracted/elpais_pages.json\", 'r', encoding='utf-8') as file:\n",
    "    original_json_data = json.load(file)\n",
    "    print(len(original_json_data[\"pages\"]))\n",
    "    \n",
    "for page in original_json_data[\"pages\"]:\n",
    "    link = page[\"link\"]\n",
    "    get_page_content(page, link)\n",
    "    json_data_with_content[\"pages\"].append(page)\n",
    "\t\n",
    "with open(\"data/test.json\", 'w', encoding='utf-8') as file:\n",
    "\tjson.dump(json_data_with_content, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Clinic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def verify_data(data):\n",
    "\ttry:\n",
    "\t\tdata = data.text\n",
    "\t\treturn data\n",
    "\texcept:\n",
    "\t\treturn \"None\"\n",
    "\n",
    "def get_pages(document_path):\n",
    "\n",
    "\tnum_pages = 1\n",
    "\tjson_pages_info = {\n",
    "\t\t\"pages\": []\n",
    "\t}\n",
    "\n",
    "\theaders = {'User-Agent': 'Chrome/58.0.3029.110'}\n",
    "\n",
    "\tall_pages_links = []\n",
    "\n",
    "\tfor page in range(0, num_pages):\n",
    "\t\turl = f'https://www.theclinic.cl/busqueda/?q=estallido+social'\n",
    "\t\tresponse = requests.get(url, headers=headers)\n",
    "\n",
    "\t\tif response.status_code == 200:\n",
    "\t\t\tprint(\"status 200\")\n",
    "\n",
    "\t\t\tresponse.encoding = 'utf-8'\n",
    "\t\t\t# content = response.content.decode('utf-8')\n",
    "\t\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n",
    "\t\t\t\n",
    "\t\t\twith open('output.html', 'w', encoding='utf-8') as f:\n",
    "\t\t\t\tf.write(soup.prettify())\n",
    "\n",
    "\t\t\tnews = soup.find(\"div\", class_=\"gsc-expansionArea\").find_all(\"div\", class_='gsc-webResult gsc-result')\n",
    "\n",
    "\t\t\tfor page_section in news:\n",
    "\t\t\t\tprint(\"encontrado\")\n",
    "\n",
    "\t\t\t\t# timestamp = (int)(page_section[\"data-timestamp\"])\n",
    "\t\t\t\t# if (timestamp < 1573786800 or timestamp > 1702782000):\n",
    "\t\t\t\t# \tprint(\"La noticia no corresponde a la fecha solicitada\")\n",
    "\t\t\t\t# \tcontinue\n",
    "\n",
    "\t\t\t\t# title = verify_data(page_section.find(\"header\").find(\"h2\"))\n",
    "\t\t\t\t# category = verify_data(page_section.find(\"header\").find(\"a\"))\n",
    "\t\t\t\t# description = verify_data(page_section.find(\"p\", class_=\"c_d\"))\n",
    "\t\t\t\t# date = verify_data(page_section.find(\"div\", class_=\"c_f\"))\n",
    "\t\t\t\t# image_link = page_section.find(\"figure\").find(\"a\", href=True)[\"href\"]\n",
    "\t\t\t\t# author = verify_data(page_section.find(\"div\", class_=\"c_a\"))\n",
    "\n",
    "\t\t\t\t# link_section = page_section.find(\"header\").find(\"h2\").find('a', href=True)\n",
    "\t\t\t\t# link = link_section['href']\n",
    "\n",
    "\t\t\t\t# if (all_pages_links.count(link) > 0):\n",
    "\t\t\t\t# \tprint(\"Sitio ya ingresado\")\n",
    "\t\t\t\t# \tcontinue\n",
    "\t\t\t\t# else:\n",
    "\t\t\t\t# \tall_pages_links.append(link)\n",
    "\n",
    "\t\t\t\t# link_info = {\n",
    "\t\t\t\t# \t\"newscast\" : \"El País\",\n",
    "\t\t\t\t# \t\"title\": title,\n",
    "\t\t\t\t# \t\"description\": description,\n",
    "\t\t\t\t# \t\"category\": category,\n",
    "\t\t\t\t# \t\"date\": date,\n",
    "\t\t\t\t# \t\"image_link\": image_link,\n",
    "\t\t\t\t# \t\"author\": author,\n",
    "\t\t\t\t# \t\"link\": link,\n",
    "\t\t\t\t# }\n",
    "\t\t\t\t# json_pages_info[\"pages\"].append(link_info)\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"Error al acceder a la página {page + 1}: {response.status_code}\")\n",
    "\t\t\t\n",
    "\tprint(f\"Se han encontrado: {len(json_pages_info['pages'])} páginas\")\n",
    "\n",
    "\twith open(f\"{document_path}\", 'w', encoding='utf-8') as file:\n",
    "\t\tjson.dump(json_pages_info, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pages('./pages_extracted/theClinic_pages.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN en Español\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def verify_data(data):\n",
    "\ttry:\n",
    "\t\tdata = data.text\n",
    "\t\treturn data\n",
    "\texcept:\n",
    "\t\treturn \"None\"\n",
    "\n",
    "def get_pages(document_path):\n",
    "\n",
    "\tnum_pages = 1\n",
    "\tjson_pages_info = {\n",
    "\t\t\"pages\": []\n",
    "\t}\n",
    "\n",
    "\theaders = {'User-Agent': 'Chrome/58.0.3029.110'}\n",
    "\n",
    "\tall_pages_links = []\n",
    "\n",
    "\tfor page in range(0, num_pages):\n",
    "\t\turl = f\"https://cnnespanol.cnn.com/search?q=Estallido+Social+Chile&from=0&size=10&page={page}&sort=newest&types=all&section=\"\n",
    "\t\tresponse = requests.get(url, headers=headers)\n",
    "\n",
    "\t\tif response.status_code == 200:\n",
    "\t\t\tprint(\"status 200\")\n",
    "\n",
    "\t\t\tresponse.encoding = 'utf-8'\n",
    "\t\t\tsoup = BeautifulSoup(response.text, 'html.parser')\n",
    "\t\t\t\n",
    "\t\t\twith open('output.html', 'w', encoding='utf-8') as f:\n",
    "\t\t\t\tf.write(soup.prettify())\n",
    "\n",
    "\t\t\t# news = soup.find(\"div\", class_=\"container__field-links container_list-images-with-description__field-links\").find_all(\"div\", class_=\"card2\")\n",
    "\n",
    "\t\t\t# for page_section in news:\n",
    "\t\t\t# \tprint(\"encontrado\")\n",
    "\n",
    "\t\t\t\t# timestamp = (int)(page_section[\"data-timestamp\"])\n",
    "\t\t\t\t# if (timestamp < 1573786800 or timestamp > 1702782000):\n",
    "\t\t\t\t# \tprint(\"La noticia no corresponde a la fecha solicitada\")\n",
    "\t\t\t\t# \tcontinue\n",
    "\n",
    "\t\t\t\t# title = verify_data(page_section.find(\"header\").find(\"h2\"))\n",
    "\t\t\t\t# category = verify_data(page_section.find(\"header\").find(\"a\"))\n",
    "\t\t\t\t# description = verify_data(page_section.find(\"p\", class_=\"c_d\"))\n",
    "\t\t\t\t# date = verify_data(page_section.find(\"div\", class_=\"c_f\"))\n",
    "\t\t\t\t# image_link = page_section.find(\"figure\").find(\"a\", href=True)[\"href\"]\n",
    "\t\t\t\t# author = verify_data(page_section.find(\"div\", class_=\"c_a\"))\n",
    "\n",
    "\t\t\t\t# link_section = page_section.find(\"header\").find(\"h2\").find('a', href=True)\n",
    "\t\t\t\t# link = link_section['href']\n",
    "\n",
    "\t\t\t\t# if (all_pages_links.count(link) > 0):\n",
    "\t\t\t\t# \tprint(\"Sitio ya ingresado\")\n",
    "\t\t\t\t# \tcontinue\n",
    "\t\t\t\t# else:\n",
    "\t\t\t\t# \tall_pages_links.append(link)\n",
    "\n",
    "\t\t\t\t# link_info = {\n",
    "\t\t\t\t# \t\"newscast\" : \"El País\",\n",
    "\t\t\t\t# \t\"title\": title,\n",
    "\t\t\t\t# \t\"description\": description,\n",
    "\t\t\t\t# \t\"category\": category,\n",
    "\t\t\t\t# \t\"date\": date,\n",
    "\t\t\t\t# \t\"image_link\": image_link,\n",
    "\t\t\t\t# \t\"author\": author,\n",
    "\t\t\t\t# \t\"link\": link,\n",
    "\t\t\t\t# }\n",
    "\t\t\t\t# json_pages_info[\"pages\"].append(link_info)\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"Error al acceder a la página {page + 1}: {response.status_code}\")\n",
    "\t\t\t\n",
    "\tprint(f\"Se han encontrado: {len(json_pages_info['pages'])} páginas\")\n",
    "\n",
    "\twith open(f\"{document_path}\", 'w', encoding='utf-8') as file:\n",
    "\t\tjson.dump(json_pages_info, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pages('./pages_extracted/cnnEspañol_pages.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Configurar opciones para Chrome (modo headless)\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\") \n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "for page in range(1, 3):\n",
    "\tprint(page)\n",
    "\n",
    "\tdriver = webdriver.Chrome(options=options)\n",
    "\n",
    "\tfrom_value = (page - 1) * 10\n",
    "\turl = f\"https://cnnespanol.cnn.com/search?q=Estallido+Social+Chile&from={from_value}&size=10&sort=newest&types=all&section=\"\n",
    "\n",
    "\tdriver.get(url)\n",
    "\n",
    "\t# Esperar a que la página cargue completamente\n",
    "\ttime.sleep(3)\n",
    "\n",
    "\twith open(f\"output{page}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "\t\t\tf.write(driver.page_source)\n",
    "\tdriver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La Tercera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_pages_info = {\"pages\": []}\n",
    "\n",
    "# Configurar opciones para Chrome (modo headless)\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\") \n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# from_value = (page - 1) * 10\n",
    "url = f\"https://www.latercera.com/search/?q=Estallido%20Social%20Chile\"\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Esperar a que la página cargue completamente\n",
    "time.sleep(3)\n",
    "\n",
    "elements = driver.find_element(By.CLASS_NAME, \"gsc-expansionArea\")\n",
    "articles = elements.find_elements(By.CLASS_NAME, \"gsc-webResult\")\n",
    "\n",
    "print(f\"Se han encontrado: {len(articles)} páginas\")\n",
    "\n",
    "\n",
    "for article in articles:\n",
    "\tprint(\"encontrado\")\n",
    "\ttitle = article.find_element(By.CLASS_NAME, \"gs-title\").text\n",
    "\tdescription = article.find_element(By.CLASS_NAME, \"gs-snippet\").text\n",
    "\tdate = description[0:11]\n",
    "\tlink = article.find_element(By.CLASS_NAME, \"gs-title\").find_element(By.CLASS_NAME, \"gs-title\").get_attribute(\"href\")\n",
    "\n",
    "\tlink_info = {\n",
    "\t\t\"newscast\" : \"La Tercera\",\n",
    "\t\t\"title\": title,\n",
    "\t\t\"description\": description,\n",
    "\t\t\"date\": date,\n",
    "\t\t\"link\": link,\n",
    "\t}\n",
    "\n",
    "\t# print(f\"Title: {title}\")\n",
    "\t# print(f\"Description: {description}\")\n",
    "\t# print(f\"Date: {date}\")\n",
    "\t# print(f\"Link: {link}\")\n",
    "\tjson_pages_info[\"pages\"].append(link_info)\n",
    "\n",
    "\n",
    "pages_list = driver.find_element(By.CLASS_NAME, \"gsc-cursor\")\n",
    "pages_links = pages_list.find_elements(By.CLASS_NAME, \"gsc-cursor-page\")\n",
    "\n",
    "print(f\"Se han encontrado: {len(pages_links)} links\")\n",
    "\n",
    "pages_links[0].click()\n",
    "\n",
    "# Esperar a que la página cargue completamente\n",
    "time.sleep(3)\n",
    "\n",
    "elements = driver.find_element(By.CLASS_NAME, \"gsc-expansionArea\")\n",
    "articles = elements.find_element(By.CLASS_NAME, \"gsc-webResult\")\n",
    "\n",
    "print(f\"Se han encontrado: {len(articles)} páginas\")\n",
    "\n",
    "with open(f\"output.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "\t\tf.write(driver.page_source)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "with open(f\"./pages_extracted/laTercera_pages.json\", 'w', encoding='utf-8') as file:\n",
    "\tjson.dump(json_pages_info, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "url = \"https://cse.google.com/cse/element/v1?rsz=filtered_cse&num=10&hl=es&source=gcsc&start=30&cselibv=75c56d121cde450a&cx=0099769ad8b6d3d5e&q=Estallido+Social+Chile&safe=off&cse_tok=AB-tC_6ZjKQAJAOQmh9snYkP7ics%3A1746614694780&exp=cc%2Capo&callback=google.search.cse.api6184&rurl=https%3A%2F%2Fwww.latercera.com%2Fsearch%2F%3Fq%3DEstallido%2520Social%2520Chile\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    match = re.search(r'^[^(]+\\((.*)\\);?$', response.text)\n",
    "    if match:\n",
    "        json_str = match.group(1)\n",
    "        data = json.loads(json_str)\n",
    "        print(data)\n",
    "    else:\n",
    "        print(\"No se pudo extraer el JSON.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
